{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.functions import isnan, split, element_at, when, col, count, isnull\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, OneHotEncoder\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "mtrip = ss.sql(\"select * from parquet.`s3a://usf.msds.stillbikeshare/merged`\")\n",
    "mtrip.cache()\n",
    "weather = ss.read.csv(\"s3a://usf.msds.stillbikeshare/weather.csv\", inferSchema=True, header=True)\n",
    "weather.cache()\n",
    "\n",
    "columns = weather.schema.names\n",
    "for x in ['date', 'precipitation_inches', 'events', 'zip_code']:\n",
    "    columns.remove(x) \n",
    "\n",
    "columns\n",
    "\n",
    "for col in columns:\n",
    "    mtrip_df = mtrip.withColumn(col, mtrip[col].cast(FloatType()))\n",
    "mtrip\n",
    "\n",
    "# use median strategy to impute numerical weather columns\n",
    "imputed_columns = [\"out_\"+col for col in columns]\n",
    "imputer = Imputer(inputCols=columns, outputCols=imputed_columns).setStrategy(\"median\")\n",
    "mtrip_weather_imputed_part_one = imputer.fit(mtrip)\n",
    "mtrip_imp_weather_part_one = mtrip_weather_imputed_part_one.transform(mtrip)\n",
    "mtrip_imp_weather_part_one.schema.names\n",
    "\n",
    "# check whether out_max_gust_speed_mph has been successfully imputed\n",
    "mtrip_imp_weather_part_one.select('out_max_gust_speed_mph').show()\n",
    "mtrip_imp_weather_part_one.select([count(when(isnan('out_max_gust_speed_mph'), 'out_max_gust_speed_mph')).alias('count_max_gust_speed_mph')]).show()\n",
    "\n",
    "# determine the most frequent value in precipitation inches\n",
    "mtrip_imp_weather_part_one.groupBy('precipitation_inches').count().orderBy('count', ascending=False).show()\n",
    "\n",
    "# use mode strategy to impute precipitation_inches\n",
    "mtrip_weather_imputed_fin = mtrip_imp_weather_part_one.fillna({'precipitation_inches':'0'})\n",
    "mtrip_imp_weather_part_one.select([count(when(isnan('precipitation_inches'), 'precipitation_inches')).alias('count_precipitation_inches')]).show()\n",
    "\n",
    "for col in columns:\n",
    "    mtrip_weather_imputed_fin = mtrip_weather_imputed_fin.drop(col)\n",
    "type(mtrip_weather_imputed_fin.schema['start_city'].dataType)\n",
    "\n",
    "mtrip_weather_imputed_fin\n",
    "\n",
    "# select all the names of the string type columns\n",
    "string_names = []\n",
    "for name in mtrip_weather_imputed_fin.schema.names:\n",
    "    if mtrip_weather_imputed_fin.schema[name].dataType == StringType():\n",
    "        string_names.append(name)\n",
    "string_names\n",
    "\n",
    "string_names_without_date = ['start_station_name', 'end_station_name', 'subscription_type', 'precipitation_inches', 'events', 'start_name', 'start_city', 'end_name', 'end_city']\n",
    "indexed_string_names_without_date = ['ind_' + name for name in string_names_without_date]\n",
    "\n",
    "for i in range(len(string_names_without_date)):\n",
    "    indexed_train_fit = StringIndexer(inputCol=string_names_without_date[i], \\\n",
    "                                      outputCol=indexed_string_names_without_date[i]).setHandleInvalid(\"keep\")\\\n",
    "                        .fit(mtrip_weather_imputed_fin)\n",
    "    mtrip_weather_imputed_fin = indexed_train_fit.transform(mtrip_weather_imputed_fin)\\\n",
    "                        .drop(string_names_without_date[i]).withColumnRenamed(indexed_string_names_without_date[i], string_names_without_date[i])\n",
    "\n",
    "# successfully string indexed\n",
    "mtrip_weather_imputed_fin.select('subscription_type', 'precipitation_inches').show(10)\n",
    "\n",
    "set(string_names).difference(set(string_names_without_date))\n",
    "\n",
    "# encode start_date and end_date\n",
    "from pyspark.sql.functions import udf,desc\n",
    "from datetime import datetime\n",
    "\n",
    "# weekday\n",
    "start_weekDay =  udf(lambda x: datetime.strptime(x, '%m/%d/%Y %H:%M').strftime('%w'))\n",
    "mtrip_weather_imputed_fin = mtrip_weather_imputed_fin.withColumn('start_weekday', start_weekDay(mtrip_weather_imputed_fin['start_date'])).sort(desc(\"start_date\"))\n",
    "end_weekDay =  udf(lambda x: datetime.strptime(x, '%m/%d/%Y %H:%M').strftime('%w'))\n",
    "mtrip_weather_imputed_fin = mtrip_weather_imputed_fin.withColumn('end_weekday', end_weekDay(mtrip_weather_imputed_fin['end_date'])).sort(desc(\"end_date\"))\n",
    "\n",
    "# month\n",
    "start_month =  udf(lambda x: datetime.strptime(x, '%m/%d/%Y %H:%M').strftime('%m'))\n",
    "mtrip_weather_imputed_fin = mtrip_weather_imputed_fin.withColumn('start_month', start_month(mtrip_weather_imputed_fin['start_date'])).sort(desc(\"start_date\"))\n",
    "end_month =  udf(lambda x: datetime.strptime(x, '%m/%d/%Y %H:%M').strftime('%m'))\n",
    "mtrip_weather_imputed_fin = mtrip_weather_imputed_fin.withColumn('end_month', end_month(mtrip_weather_imputed_fin['end_date'])).sort(desc(\"end_date\"))\n",
    "\n",
    "# hour\n",
    "start_hour =  udf(lambda x: datetime.strptime(x, '%m/%d/%Y %H:%M').strftime('%H'))\n",
    "mtrip_weather_imputed_fin = mtrip_weather_imputed_fin.withColumn('start_hour', start_hour(mtrip_weather_imputed_fin['start_date'])).sort(desc(\"start_date\"))\n",
    "end_hour =  udf(lambda x: datetime.strptime(x, '%m/%d/%Y %H:%M').strftime('%H'))\n",
    "mtrip_weather_imputed_fin = mtrip_weather_imputed_fin.withColumn('end_hour', end_hour(mtrip_weather_imputed_fin['end_date'])).sort(desc(\"end_date\"))\n",
    "\n",
    "mtrip_weather_imputed_fin.select('end_hour').show()\n",
    "\n",
    "# change all str date related columns to integer type\n",
    "for name in [\"start_weekday\", \"end_weekday\", \"start_month\", \"end_month\", \"start_hour\", \"end_hour\"]:\n",
    "    mtrip_weather_imputed_fin = mtrip_weather_imputed_fin.withColumn(name, mtrip_weather_imputed_fin[name].cast(IntegerType()))\n",
    "\n",
    "mtrip_weather_imputed_fin = mtrip_weather_imputed_fin.drop('end_installation_date', 'start_date', 'start_installation_date', 'date', 'start_date_no_time', 'end_date')\n",
    "\n",
    "mtrip_weather_imputed_fin\n",
    "\n",
    "# one hot encode weekday month hour\n",
    "\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "df_mtrip_spark = oneHotEncodeColumns(mtrip_weather_imputed_fin, [\"start_weekday\", \"end_weekday\", \"start_month\", \"end_month\", \"start_hour\", \"end_hour\"])                          \n",
    "\n",
    "df_mtrip_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mtrip_spark.write.parquet(\"s3a://usf.msds.stillbikeshare/clean_data_bike_share\")\n",
    "\n",
    "df_mtrip_read = ss.sql(\"select * from parquet.`s3a://usf.msds.stillbikeshare/clean_data_bike_share`\")\n",
    "df_mtrip_read.show()\n",
    "\n",
    "sc.install_pypi_package(\"pandas==0.25.1\") #Install pandas version 0.25.1 \n",
    "sc.install_pypi_package(\"matplotlib\", \"https://pypi.org/simple\") \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_mtrip = mtrip_weather_imputed_fin.toPandas()\n",
    "\n",
    "df_mtrip = pd.concat([df_mtrip, pd.get_dummies(df_mtrip['start_weekday'], prefix='start_weekday')], axis=1)\n",
    "df_mtrip = pd.concat([df_mtrip, pd.get_dummies(df_mtrip['end_weekday'], prefix='end_weekday')], axis=1)\n",
    "\n",
    "df_mtrip\n",
    "\n",
    "# one hot encode weekday\n",
    "mtrip_weather_imputed_fin.show()\n",
    "\n",
    "model = encoder.fit(mtrip_weather_imputed_fin)\n",
    "\n",
    "encoded = model.transform(mtrip_weather_imputed_fin)\n",
    "\n",
    "encoded\n",
    "\n",
    "# I don't think the date in the weather df making as much sense since our merged dataset based on zipcode\n",
    "# similarly the installation dates not making as much sense either\n",
    "# thus start_date and end_State are more reasonable predictors directly related to our trip\n",
    "mtrip_weather_imputed_fin = mtrip_weather_imputed_fin.drop('start_installation_date', 'end_installation_date', 'date')\n",
    "\n",
    "mtrip_weather_imputed_fin.select('end_hour').distinct().show()\n",
    "\n",
    "mtrip_weather_imputed_fin.select('weekdays')\n",
    "\n",
    "mtrip_weather_imputed_fin.write.save(\"s3a://usf.msds.stillbikeshare/cleaned_bike_share.csv\", format='csv', header=True)\n",
    "\n",
    "mtrip_weather_imputed_fin.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
